# classes/ai_compliance.py

import numpy as np
import pandas as pd
import re

class AIComplianceAnalyzer:
    def __init__(self, df: pd.DataFrame):
        self.df = df.copy()

    def analyze_bias(self, group_cols=None, target_cols=None):
        """
        Wykrywa potencjalne uprzedzenia (bias) wzglÄ™dem kolumn grupujÄ…cych.
        """
        if group_cols is None:
            group_cols = ["COUNTRYNAME", "ChannelName", "PaymentMethodName"]
        if target_cols is None:
            target_cols = ["TRANSACTIONPRICE", "DISCOUNTPCTG"]

        report = {}

        for group_col in group_cols:
            if group_col in self.df.columns:
                try:
                    report[group_col] = self._compute_bias_for_column(group_col, target_cols)
                except Exception as e:
                    report[group_col] = f"BÅ‚Ä…d analizy: {e}"

        return report

    def _compute_bias_for_column(self, group_col, target_cols):
        bias_report = {}

        value_counts = self.df[group_col].value_counts(normalize=True)
        entropia = -np.sum(value_counts * np.log2(value_counts + 1e-9))
        prop_diff = value_counts.max() - value_counts.min()

        bias_report["RozkÅ‚ad kategorii"] = {
            "Entropia": round(entropia, 4),
            "Max udziaÅ‚": round(value_counts.max(), 4),
            "Min udziaÅ‚": round(value_counts.min(), 4),
            "RozstÄ™p udziaÅ‚Ã³w": round(prop_diff, 4),
            "Liczba grup": len(value_counts)
        }

        for target in target_cols:
            if target in self.df.columns:
                grouped = self.df.groupby(group_col)[target].mean()
                bias_report[f"Åšrednia {target} wg grup"] = grouped.to_dict()
                bias_report[f"RozstÄ™p Å›rednich {target}"] = round(grouped.max() - grouped.min(), 4)

        return bias_report

    def analyze_sensitive_data(self):
        """
        Wykrywa dane osobowe i wraÅ¼liwe oraz ocenia poziom ryzyka, z uwzglÄ™dnieniem wyjÄ…tkÃ³w.
        """
        sensitive_keywords = ['health', 'gender', 'religion', 'ethnicity', 'politics', 'income', 'disability', 'sexual']
        personal_keywords = ['name', 'firstname', 'lastname', 'email', 'phone', 'address', 'dob', 'pesel', 'nip', 'user', 'ip']

        # Kolumny, ktÃ³re majÄ… sÅ‚owo 'name', ale nie sÄ… osobowe
        name_exceptions = ['productname', 'paymentmethodname', 'deliverymethodname', 
                        'channelname', 'productsubcategoryname', 'productcategoryname']

        sensitive_cols = []
        personal_cols = []

        for col in self.df.columns:
            col_lower = col.lower()

            if col_lower in name_exceptions:
                continue  # pomijamy mylÄ…ce nazwy

            if any(kw in col_lower for kw in sensitive_keywords):
                sensitive_cols.append(col)
            if any(kw in col_lower for kw in personal_keywords):
                personal_cols.append(col)

        high_risk = bool(sensitive_cols or personal_cols)

        if high_risk:
            risk_level = "ğŸš¨ Wysokie - dane wraÅ¼liwe lub identyfikujÄ…ce"
        elif any("id" in col.lower() for col in self.df.columns if col.lower() not in name_exceptions):
            risk_level = "âš ï¸ Åšrednie - dane pseudonimizowane"
        else:
            risk_level = "âœ… Niskie - brak danych osobowych/wraÅ¼liwych"

        return {
            "Dane osobowe": personal_cols,
            "Dane wraÅ¼liwe": sensitive_cols,
            "Poziom ryzyka": risk_level
        }
    
    def get_data_lineage(self):
        """
        Zwraca informacje o pochodzeniu danych: ÅºrÃ³dÅ‚o kolumny i typ pochodzenia (oryginalna, join, wyliczona).
        """
        lineage_map = {
            # Oryginalne kolumny z tabeli faktÃ³w
            "ORDERKEY": ("FactOnlineSales", "oryginalna"),
            "ORDERLINENUMBER": ("FactOnlineSales", "oryginalna"),
            "TRANSACTIONPRICE": ("FactOnlineSales", "oryginalna"),
            "QUANTITY": ("FactOnlineSales", "oryginalna"),
            "DISCOUNTPCTG": ("FactOnlineSales", "oryginalna"),
            "DELIVERYCOST": ("FactOnlineSales", "oryginalna"),
            "PRODUCTCOST": ("FactOnlineSales", "oryginalna"),

            # Kolumny z joinÃ³w
            "CustomerName": ("DimCustomer", "join przez CustomerKey + przeksztaÅ‚cenie (imiÄ™ + nazwisko)"),
            "ProductName": ("DimProduct", "join przez ProductKey"),
            "ProductCategoryName": ("DimProduct", "join przez ProductKey"),
            "ChannelName": ("DimOrderChannel", "join przez ChannelKey"),
            "PaymentMethodName": ("DimPaymentMethod", "join przez PaymentMethodKey"),
            "DeliveryMethodName": ("DimDeliveryMethod", "join przez DeliveryMethodKey"),
            "COUNTRYNAME": ("DimSalesTerritory", "join przez SalesTerritoryKey"),
            "ProductSubcategoryName": ("DimProduct", "join przez ProductKey"),

            # Kolumny wyliczone
            "TotalTransactionPrice": ("FactOnlineSales", "wyliczona (TransactionPrice x Quantity)"),
            "TotalDiscountAmount": ("FactOnlineSales", "wyliczona (DiscountAmount x Quantity)"),
            "TotalCatalogPrice": ("FactOnlineSales", "wyliczona (CatalogPrice x Quantity)")
        }

        lineage_info = []
        for col in self.df.columns:
            source, col_type = lineage_map.get(
                col, ("â“ Nieznane", "â“ NieokreÅ›lone"))
            lineage_info.append({
                "Kolumna": col,
                "Å¹rÃ³dÅ‚o": source,
                "Typ pochodzenia": col_type
            })

        return lineage_info
    
    def evaluate_risk(self):
        """
        Ocena koÅ„cowego ryzyka zgodnoÅ›ci danych z AI Act.
        Analizuje: prywatnoÅ›Ä‡, stronniczoÅ›Ä‡ i pochodzenie danych.
        """
        # --- PrywatnoÅ›Ä‡
        privacy = self.analyze_sensitive_data()
        privacy_level = privacy["Poziom ryzyka"]

        if "wysokie" in privacy_level.lower():
            privacy_score = 2
        elif "Å›rednie" in privacy_level.lower():
            privacy_score = 1
        else:
            privacy_score = 0

        # --- StronniczoÅ›Ä‡ (na podstawie rozstÄ™pÃ³w udziaÅ‚Ã³w)
        bias = self.analyze_bias()
        bias_scores = []

        for col, report in bias.items():
            if isinstance(report, dict):
                spread = report["RozkÅ‚ad kategorii"]["RozstÄ™p udziaÅ‚Ã³w"]
                if spread > 0.5:
                    bias_scores.append(2)
                elif spread > 0.2:
                    bias_scores.append(1)
                else:
                    bias_scores.append(0)

        bias_score = max(bias_scores) if bias_scores else 0

        # --- Pochodzenie danych
        lineage = self.get_data_lineage()
        unknowns = [entry for entry in lineage if "â“" in entry["Å¹rÃ³dÅ‚o"]]
        lineage_score = 2 if len(unknowns) > 3 else 1 if unknowns else 0

        total_score = privacy_score + bias_score + lineage_score

        if total_score >= 5:
            overall = "ğŸš¨ Wysokie ryzyko"
        elif total_score >= 3:
            overall = "âš ï¸ Åšrednie ryzyko"
        else:
            overall = "âœ… Niskie ryzyko"

        return {
            "Ocena ogÃ³lna": overall,
            "PrywatnoÅ›Ä‡": privacy_level,
            "StronniczoÅ›Ä‡": f"{['âœ… Niska', 'âš ï¸ Åšrednia', 'ğŸš¨ Wysoka'][bias_score]}",
            "Pochodzenie danych": f"{['âœ… Znane', 'âš ï¸ CzÄ™Å›ciowo nieznane', 'ğŸš¨ Braki w definicji'][lineage_score]}"
        }


